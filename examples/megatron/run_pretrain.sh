#!/bin/bash
# shellcheck disable=SC2086
###############################################################################
# Copyright (c) 2025, Advanced Micro Devices, Inc. All rights reserved.
#
# See LICENSE for license information.
#################################################################################

# Usage Guide: run_pretrain.sh
#
# Description:
#   This script sets up environment variables, configures cluster and GPU
#   settings, prepares datasets, and launches Megatron-LM training under the
#   Primus framework. It supports single-node and multi-node distributed
#   training, and includes support for AMD ROCm platforms.
#
# Usage:
#   EXP=examples/megatron/exp_pretrain.yaml ./run_pretrain.sh
#
# Environment Variables:
#   Required:
#     - EXP: Path to the experiment YAML configuration.
#
#   Optional:
#     - MEGATRON_PATH: Path to Megatron-LM repo (default: $PRIMUS_PATH/third_party/Megatron-LM-*)
#
#     - DATA_PATH: Root path to datasets (default: $PRIMUS_PATH/data)
#     - HF_HOME: Huggingface cache directory (default: $DATA_PATH/huggingface)
#     - TOKENIZED_DATA_PATH: Path to tokenized dataset (auto-generated if not set)
#     - HF_TOKEN: Huggingface token for downloading models
#
#     - PRIMUS_HIPBLASLT_TUNING_STAGE:
#       0: No tuning (default)
#       1: Dump GEMM shapes (use small train_iters)
#       2: Run offline tuning based on dumped shapes
#       3: Use tuned config for optimized HipBLASLt execution
#
# Multi-node Settings:
#   - MASTER_ADDR: IP address of master node (default: localhost)
#   - MASTER_PORT: Port of master node (default: 1234)
#   - NNODES: Number of total nodes (default: 1)
#   - NODE_RANK: Rank of the current node (default: 0)
#   - GPUS_PER_NODE: Number of GPUs per node (default: 8)
#
# ï¸Notes for Multi-node Training:
#   - The following paths must reside on a **shared filesystem** (e.g., NFS):
#     1. PRIMUS_PATH (this repo, including Megatron-LM submodule, scripts, logs)
#     2. DATA_PATH
#     3. HF_HOME
#     4. TOKENIZED_DATA_PATH
#     All nodes must be able to access the same files via the same absolute path.
#
#   - Megatron-LM submodule must be initialized:
#       git submodule update --init --recursive
#
#   - Tokenized dataset will be automatically generated by rank 0 if not present.
#   - HipBLASLt tuning is optional but recommended for AMD GPUs.
#
# Example:
#   PRIMUS_HIPBLASLT_TUNING_STAGE=1 EXP=examples/megatron/exp_pretrain.yaml ./run_pretrain.sh
###############################################################################


# ----------- Framework Paths -----------
# Setup essential Python paths for Megatron-LM and Primus,
# ensuring all dependencies are correctly discoverable during execution.

# Set PRIMUS_PATH to the root directory of the framework
PRIMUS_PATH=$(realpath "$(dirname "$0")/../..")
echo "[INFO] PRIMUS_PATH is set to: ${PRIMUS_PATH}"

# Set MEGATRON_PATH to the default path unless explicitly provided
export MEGATRON_PATH=${MEGATRON_PATH:-${PRIMUS_PATH}/third_party/Megatron-LM-20250324}
echo "[INFO] MEGATRON_PATH is set to: ${MEGATRON_PATH}"

# Validate that MEGATRON_PATH exists and is not empty
if [[ ! -d "$MEGATRON_PATH" || -z "$(ls -A "$MEGATRON_PATH")" ]]; then
    echo "[ERROR] MEGATRON_PATH (${MEGATRON_PATH}) does not exist or is empty."
    echo "        Please ensure Primus is properly initialized."
    echo
    echo "        If not yet cloned, run:"
    echo "            git clone --recurse-submodules git@github.com:AMD-AIG-AIMA/Primus.git"
    echo
    echo "        Or if already cloned, initialize submodules with:"
    echo "            git submodule update --init --recursive"
    echo
    exit 1
fi

export DATA_PATH=${DATA_PATH:-"${PRIMUS_PATH}/data"}
echo "[INFO] DATA_PATH is set to: ${DATA_PATH}"

export HF_HOME=${HF_HOME:-"${DATA_PATH}/huggingface"}
echo "[INFO] HF_HOME is set to: ${HF_HOME}"


# ----------- Basic Framework Configuration -----------
# Load experiment configuration, model definition, and
# tokenizer settings from YAML files. These settings
# govern model architecture and tokenizer behavior.

# Ensure EXP is set, otherwise exit with error
if [ -z "${EXP:-}" ]; then
  echo "Error: EXP must be specified (e.g., examples/megatron/exp_pretrain.yaml)."
  echo "Primus will use the configuration in EXP to train the model."
  exit 1
fi

# Ensure EXP file exists, otherwise exit with error
if [ ! -f "${EXP}" ]; then
  echo "[ERROR] The specified EXP file does not exist: ${EXP}"
  echo "        Primus will use the configuration in EXP to train the model."
  exit 1
fi
echo "[INFO] EXP is set to: ${EXP}"


EXP_NAME=$(basename "$EXP" .yaml)
TRAIN_LOG="output/log_torchrun_pretrain_${EXP_NAME}.txt"

if [ "$NODE_RANK" = "0" ]; then
    echo "==========Training info=========="
    echo "[NODE-$NODE_RANK] EXP: $EXP"
    echo "[NODE-$NODE_RANK] TRAIN_LOG: $TRAIN_LOG"
    echo ""
fi


# ----------- Cluster Configuration -----------
# Define distributed training parameters such as number of nodes,
# rank of each node, and master address/port for communication.


export MASTER_ADDR=${MASTER_ADDR:-localhost}
export MASTER_PORT=${MASTER_PORT:-1234}
export NNODES=${NNODES:-1}
export NODE_RANK=${NODE_RANK:-0}
export GPUS_PER_NODE=${GPUS_PER_NODE:-8}

HOSTNAME=$(hostname)
if [ "$NODE_RANK" = "0" ]; then
    echo "==========Training cluster info=========="
    echo "[NODE-$NODE_RANK($HOSTNAME)] MASTER_ADDR: $MASTER_ADDR"
    echo "[NODE-$NODE_RANK($HOSTNAME)] MASTER_PORT: $MASTER_PORT"
    echo "[NODE-$NODE_RANK($HOSTNAME)] NNODES: $NNODES"
    echo "[NODE-$NODE_RANK($HOSTNAME)] NODE_RANK: $NODE_RANK"
    echo "[NODE-$NODE_RANK($HOSTNAME)] GPUS_PER_NODE: $GPUS_PER_NODE"
    echo ""
fi


# ----------- GPU and Communication Settings -----------
# Configure GPU-related environment variables and communication backend
# for efficient distributed training across multiple devices.

# Set visible GPUs for the current node (0 to GPUS_PER_NODE-1)
HIP_VISIBLE_DEVICES=$(seq -s, 0 $((GPUS_PER_NODE - 1)))
export HIP_VISIBLE_DEVICES

# ----------------- NCCL and Network Settings -----------------

# Set InfiniBand GID index for NCCL communication
export NCCL_IB_GID_INDEX=3

# Disable cross NIC communication for NCCL
export NCCL_CROSS_NIC=0

# Dynamically get InfiniBand Host Channel Adapter index for NCCL
NCCL_IB_HCA=$(bash "${PRIMUS_PATH}/examples/scripts/get_nccl_ib_hca.sh")
export NCCL_IB_HCA

# Dynamically get network interface IP address for socket communication
IP_INTERFACE=$(bash "${PRIMUS_PATH}/examples/scripts/get_ip_interface.sh")
export IP_INTERFACE

# Set network interfaces for NCCL and Gloo, fallback to detected IP_INTERFACE
export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:-$IP_INTERFACE}
export GLOO_SOCKET_IFNAME=${GLOO_SOCKET_IFNAME:-$IP_INTERFACE}

# ----------------- AMD-specific GPU optimizations -----------------

# Enable system DMA engine (SDMA) on AMD GPUs for better IO throughput
export HSA_ENABLE_SDMA=1

# Prevent scratch memory from being reclaimed to stabilize large memory usage patterns (e.g., KV cache, MoE experts)
export HSA_NO_SCRATCH_RECLAIM=1

# Disable MSCCL (RCCL multi-connection feature) for better stability
export RCCL_MSCCL_ENABLE=0

# ----------------- Performance tuning -----------------

# Disable NCCL internal checks to reduce overhead
export NCCL_CHECKS_DISABLE=1

# Limit GPU hardware queues to 2 for performance stability
export GPU_MAX_HW_QUEUES=2

# Limit max CUDA device connections to reduce PCIe traffic
export CUDA_DEVICE_MAX_CONNECTIONS=1

# Prioritize NCCL communication for PyTorch for higher throughput
export TORCH_NCCL_HIGH_PRIORITY=1

# ----------------- NVTE FP8 -----------------
export NVTE_USE_CAST_TRANSPOSE_TRITON=1
# export NVTE_USE_OPTIMIZED_HIPIFIED_CAST_TRANSPOSE=1


# ----------- HipBLASLt Tuning -----------
# Configure HipBLASLt tuning stage to either dump GEMM shapes for profiling
# or apply tuned configurations for optimized GEMM execution on AMD GPUs.

handle_hipblaslt_tuning() {
    local STAGE=${PRIMUS_HIPBLASLT_TUNING_STAGE:-0}
    local TUNE_LOG_PATH=${PRIMUS_PATH}/output/tune_hipblaslt/${MODEL}
    local RESULT_FILE=tune_hipblas_gemm_results.txt

    mkdir -p "$TUNE_LOG_PATH"

    case $STAGE in
        1)
            [[ "$TE_HIPBLASLT_TUNING" == "1" ]] && error_exit "Disable TE_HIPBLASLT_TUNING for shape dump"
            mkdir -p "$TUNE_LOG_PATH/gemm_shape"
            export HIPBLASLT_LOG_MASK=32
            export HIPBLASLT_LOG_FILE=${HIPBLASLT_LOG_FILE:-"$TUNE_LOG_PATH/gemm_shape/dump_hipblaslt_gemm_shape_${NODE_RANK}.txt"}
            unset HIPBLASLT_TUNING_OVERRIDE_FILE
            ;;
        2)
            mkdir -p "$TUNE_LOG_PATH/gemm_tune"
            python "${PRIMUS_PATH}/examples/offline_tune/offline_tune_gemm.py" \
                --dump-shape-path-or-file "$TUNE_LOG_PATH/gemm_shape" \
                --tune-result-path "$TUNE_LOG_PATH/gemm_tune/$RESULT_FILE" \
                --num-devices 8
            log_info "GEMM tuning finished. Set PRIMUS_HIPBLASLT_TUNING_STAGE=3 and re-run training."
            exit 0
            ;;
        3)
            TUNE_FILE="$TUNE_LOG_PATH/gemm_tune/$RESULT_FILE"
            [[ ! -f "$TUNE_FILE" ]] && error_exit "Missing tuning result: $TUNE_FILE"
            export HIPBLASLT_TUNING_OVERRIDE_FILE=$TUNE_FILE
            ;;
    esac

    if [ "$NODE_RANK" = "0" ]; then
        echo "========== Training tuning info =========="
        echo "[NODE-$NODE_RANK] TE_HIPBLASLT_TUNING: $TE_HIPBLASLT_TUNING"
        echo "[NODE-$NODE_RANK] TE_HIPBLASLT_TUNING_RUN_COUNT: $TE_HIPBLASLT_TUNING_RUN_COUNT"
        echo "[NODE-$NODE_RANK] TE_HIPBLASLT_TUNING_ALGO_COUNT: $TE_HIPBLASLT_TUNING_ALGO_COUNT"
        echo "[NODE-$NODE_RANK] NVTE_CK_USES_BWD_V3: $NVTE_CK_USES_BWD_V3"
        echo "[NODE-$NODE_RANK] PRIMUS_HIPBLASLT_TUNING_STAGE: ${PRIMUS_HIPBLASLT_TUNING_STAGE}"
        echo "[NODE-$NODE_RANK] HIPBLASLT_LOG_MASK: ${HIPBLASLT_LOG_MASK}"
        echo "[NODE-$NODE_RANK] HIPBLASLT_LOG_FILE: ${HIPBLASLT_LOG_FILE}"
        echo "[NODE-$NODE_RANK] HIPBLASLT_LOG_LEVEL: ${HIPBLASLT_LOG_LEVEL}"
        echo "[NODE-$NODE_RANK] HIPBLASLT_TUNING_OVERRIDE_FILE: ${HIPBLASLT_TUNING_OVERRIDE_FILE}"
        if [ $STAGE -eq 1 ]; then
            echo "[NODE-$NODE_RANK] Dump HipBLASLt shapes, make sure train_iters is set to a very small value."
        fi
        echo ""
    fi
}

handle_hipblaslt_tuning



# ----------- Dataset Preparation -----------
# Prepare or validate the tokenized dataset used for training.
# Ensures consistency across nodes and performs preprocessing if necessary.


prepare_dataset() {
    MODEL=$(grep "^[[:space:]]*model:" "$EXP" | awk -F ': ' '{print $2}')
    # Check if MODEL is set, otherwise exit with error
    if [ -z "$MODEL" ]; then
        echo "[ERROR] 'model' must be specified as one of the available model names in $EXP."
        echo "        Example model: llama2_7B.yaml"
        echo "        Available models can be found under: primus/configs/models/megatron/"
        exit 1
    fi

    if [[ "$MODEL" =~ ^\$\{([^}:]+)(:([^}]+))?\}\.yaml$ ]]; then
        VAR_NAME="${BASH_REMATCH[1]}"
        DEFAULT_VALUE="${BASH_REMATCH[3]}"

        if [[ -n "${!VAR_NAME}" ]]; then
            RESOLVED_MODEL="${!VAR_NAME}"
            echo "Using environment variable $VAR_NAME = ${!VAR_NAME}, resolved MODEL = $RESOLVED_MODEL"
        elif [[ -n "$DEFAULT_VALUE" ]]; then
            RESOLVED_MODEL="${DEFAULT_VALUE}"
            echo "Using default value for $VAR_NAME = $DEFAULT_VALUE, resolved MODEL = $RESOLVED_MODEL"
        else
            echo "[ERROR] Neither environment variable \$$VAR_NAME is set nor default value provided for MODEL."
            exit 1
        fi
        MODEL=$RESOLVED_MODEL
    else
        # Not in ${VAL}.yaml or ${VAL:VALUE}.yaml format; must end with .yaml
        if [[ "$MODEL" != *.yaml ]]; then
            echo "[ERROR] MODEL must end with .yaml (given: $MODEL)"
            exit 1
        fi
        echo "Using literal MODEL = $RESOLVED_MODEL"
    fi

    MODEL_CONFIG_FILE="$PRIMUS_PATH/primus/configs/models/megatron/${MODEL}.yaml"
    if [[ ! -f "$MODEL_CONFIG_FILE" ]]; then
        echo "[ERROR] Model config file not found: $MODEL_CONFIG_FILE"
        echo "        Please make sure the file exists under primus/configs/models/megatron/"
        exit 1
    fi

    # Extract tokenizer_type and tokenizer_model from the model config file
    TOKENIZER_TYPE=$(grep "^tokenizer_type:" "$MODEL_CONFIG_FILE" | awk -F ': ' '{print $2}')
    TOKENIZER_MODEL=$(grep "^tokenizer_model:" "$MODEL_CONFIG_FILE" | awk -F ': ' '{print $2}')

    # Ensure these variables are not empty
    if [[ -z "$TOKENIZER_TYPE" ]]; then
        echo "[ERROR]: 'tokenizer_type' not found in ${MODEL_CONFIG_FILE}."
        exit 1
    fi

    if [[ -z "$TOKENIZER_MODEL" ]]; then
        echo "[ERROR]: 'tokenizer_model' not found in ${MODEL_CONFIG_FILE}."
        exit 1
    fi
    export TOKENIZER_TYPE
    export TOKENIZER_MODEL

    export TOKENIZED_DATA_PATH=${TOKENIZED_DATA_PATH:-${DATA_PATH}/bookcorpus/${TOKENIZER_TYPE}/bookcorpus_text_sentence}


    if [[ "$NODE_RANK" == "0" && ! -f "${TOKENIZED_DATA_PATH}.done" ]]; then
        # Ensure HF_TOKEN is set; exit with error if not
        if [[ -z "${HF_TOKEN}" ]]; then
            echo "Error: Environment variable HF_TOKEN must be set."
            exit 1
        fi

        bash ./examples/scripts/prepare_dataset.sh "$DATA_PATH" "$TOKENIZER_TYPE" "$TOKENIZER_MODEL"
        touch "${TOKENIZED_DATA_PATH}.done"
        log_info "Dataset preparation completed."

    elif [[ "$NODE_RANK" != "0" ]]; then
        while [[ ! -f "${TOKENIZED_DATA_PATH}.done" ]]; do
            log_info "Waiting for dataset..."
            sleep 30
        done
    fi
}

prepare_dataset


# ----------- Launch Distributed Training ------------------
# Execute the training script using torchrun with appropriate distributed
# launch arguments. Output is logged for debugging and reproducibility.


# Get the Python site-packages path dynamically
SITE_PACKAGES=$(python -c "import sysconfig; print(sysconfig.get_paths()['purelib'])")

# Set PYTHONPATH to include site-packages, Megatron, Primus, and existing PYTHONPATH
export PYTHONPATH="${SITE_PACKAGES}:${MEGATRON_PATH}:${PRIMUS_PATH}:${PYTHONPATH}"

# build helper_cpp of megatron
pushd "${MEGATRON_PATH}/megatron/core/datasets" && make && popd || exit 1

# Prepare distributed launch arguments as an array
DISTRIBUTED_ARGS=(
    --nproc_per_node "${GPUS_PER_NODE}"
    --nnodes "${NNODES}"
    --node_rank "${NODE_RANK}"
    --master_addr "${MASTER_ADDR}"
    --master_port "${MASTER_PORT}"
)

# Launch distributed training using torchrun and tee logs
torchrun "${DISTRIBUTED_ARGS[@]}" examples/megatron/pretrain.py --exp $EXP 2>&1 | tee $TRAIN_LOG
exit_code=${PIPESTATUS[0]}

if [ "${PRIMUS_HIPBLASLT_TUNING_STAGE:-0}" -eq 1 ]; then
    echo "[PRIMUS_HIPBLASLT_TUNING_STAGE-1]: HipBlasLT gemm shape dump is finished, " \
         "please set PRIMUS_HIPBLASLT_TUNING_STAGE to 2, " \
         "and tune the gemm with a single node."
fi

exit $exit_code
