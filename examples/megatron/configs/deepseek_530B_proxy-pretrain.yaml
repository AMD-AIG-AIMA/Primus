work_group: ${TEAM:amd}
user_name: ${USER:root}
exp_name: ${EXP_NAME:deepseek_530B_proxy-pretrain}
workspace: ./output

modules:
  pre_trainer:
    framework: megatron
    config: pre_trainer.yaml

    # model to run
    model: ${PRIMUS_MODEL:deepseek_530B_proxy}.yaml
    overrides:
      # log
      wandb_project: "Primus_DeepSeek_Pretrain"
      stderr_sink_level: DEBUG

      # debug
      moe_router_force_load_balancing: true
      log_avg_skip_iterations: 2
      log_avg_reset_interval: 50

      # hyber parameters
      train_iters: 8
      micro_batch_size: 1
      global_batch_size: 256
      seq_length: ${PRIMUS_SEQ_LENGTH:4096}
      max_position_embeddings: ${PRIMUS_MAX_POSITION_EMBEDDINGS:4096}
      lr: 1.0e-5
      min_lr: 0.0
      lr_warmup_iters: 2
      lr_decay_iters: null
      lr_decay_style: cosine
      weight_decay: 0.1
      adam_beta1: 0.9
      adam_beta2: 0.95
      eod_mask_loss: true
      init_method_std: 0.008
      norm_epsilon: 1.0e-6

      # # recompute
      # recompute_granularity: full # full, selective
      # recompute_method: block # uniform, block
      # recompute_num_layers: 0 # int

      # parallel
      tensor_model_parallel_size: ${PRIMUS_TP:1}
      pipeline_model_parallel_size: ${PRIMUS_PP:8}
      expert_model_parallel_size: ${PRIMUS_EP:8}
      num_virtual_stages_per_pipeline_rank: 5
      overlap_grad_reduce: true
      overlap_param_gather: true
      moe_use_legacy_grouped_gemm: true

      overlap_moe_expert_parallel_comm: true
      moe_shared_expert_overlap: false

      # data
      mock_data: true
      train_data_path: ${TOKENIZED_DATA_PATH:null}
      valid_data_path: null
      test_data_path: null

      no_save_optim: null
      no_save_rng: null
      disable_last_saving: true
      ckpt_format: torch
      eval_iters: 0

      # profile_ranks: [0, 511]
      # profile: true
      # use_pytorch_profiler: true
      # profile_step_start: 15
      # profile_step_end: 16
      

