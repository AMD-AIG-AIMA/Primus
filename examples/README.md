# üß† Pretraining with Primus

This guide demonstrates how to perform pretraining using **Megatron**/**torchtitan** within the **Primus** framework.
It supports both **single-node** and **multi-node** training, and includes optional **HipBLASLt auto-tuning** for optimal AMD GPU performance.

---

## üìö Table of Contents

- [‚öôÔ∏è Supported Backends](#Ô∏è-supported-backends)
- [üñ•Ô∏è Single Node Training](#Ô∏è-single-node-training)
  - [Setup Docker](#setup-docker)
  - [Setup Primus](#setup-primus)
  - [Run Pretraining](#run-pretraining)
- [üåê Multi-node Training](#-multi-node-training)
- [üöÄ HipBLASLt Auto Tuning (Optional)](#-hipblaslt-auto-tuning-optional)
- [‚úÖ Supported Models](#-supported-models)
  - [üèÉ‚Äç‚ôÇÔ∏è How to Run a Supported Model](#Ô∏è-how-to-run-a-supported-model)
- [‚ò∏Ô∏è Kubernetes Training Management](#Ô∏è-kubernetes-training-management-run_k8s_pretrainsh)

---

## ‚öôÔ∏è Supported Backends

Primus supports multiple backends. To specify the backend, set the `BACKEND` environment variable.

| Backend    | Description                                                  | ID (`BACKEND`) |
| ---------- | ------------------------------------------------------------ | -------------- |
| Megatron   | Open-source framework for large-scale transformer training   | `megatron`     |
| TorchTitan | PyTorch-compatible framework developed for training at scale | `torchtitan`   |


## üñ•Ô∏è Single Node Training

### Setup Docker
We recommend using the official [rocm/megatron-lm Docker image](https://hub.docker.com/r/rocm/megatron-lm) to ensure a stable and compatible training environment. Use the following commands to pull and launch the container:

```bash
# Pull the latest Docker image
docker pull docker.io/rocm/megatron-lm:v25.5_py310

```

---

### Setup Primus
Clone the repository and install dependencies:

```bash
# Clone with submodules
cd /workspace
git clone --recurse-submodules git@github.com:AMD-AIG-AIMA/Primus.git

# Or initialize submodules if already cloned
git submodule update --init --recursive

cd Primus

# Install Python dependencies
pip install -r requirements.txt

# Set up pre-commit hooks
pre-commit install
```

---

### Run Pretraining
Use the `run_pretrain.sh` script to start training.

#### üöÄ Quick Start Mode

Use this mode for **rapid iteration or validation** of a model config.
You do not need to enter the Docker container. Just set the config and run.

```bash
# Example for megatron llama3_8B
BACKEND=megatron EXP=examples/megatron/configs/llama3_8B-pretrain.yaml bash ./examples/run_local_pretrain.sh

# examples for torchtatin llama3_8b
BACKEND=torchtatin EXP=examples/torchtitan/configs/llama3_8b.toml bash ./examples/run_local_pretrain.sh
```

---

#### üßë‚Äçüîß Interactive Mode

This mode is recommended for **development, debugging**, or running **custom workflows**.
You will manually enter the container and execute training inside.

```bash
# Launch the container
bash tools/docker/start_container.sh

# Access the container
docker exec -it dev_primus bash

# install required packages
cd Primus && pip install -r requirements.txt

# Example for megatron llama3_8B
BACKEND=megatron EXP=examples/megatron/configs/llama3_8B-pretrain.yaml bash ./examples/run_pretrain.sh

# examples for torchtatin llama3_8b
BACKEND=torchtatin EXP=examples/torchtitan/configs/llama3_8b.toml bash ./examples/run_pretrain.sh

```

---

## üåê Multi-node Training

Multi-node training is launched via **SLURM**.
Specify the number of nodes and the model config:

```bash
export DOCKER_IMAGE="docker.io/rocm/megatron-lm:v25.5_py310"
export NNODES=8

# Example for megatron llama3_8B
BACKEND=megatron EXP=examples/megatron/configs/llama3_8B-pretrain.yaml bash ./examples/run_slurm_pretrain.sh

# examples for torchtatin llama3_8b
BACKEND=torchtatin EXP=examples/torchtitan/configs/llama3_8b.toml bash ./examples/run_slurm_pretrain.sh
```

## üîß HipblasLT Auto Tuning

HipblasLT tuning is divided into three stages and controlled via the environment variable `PRIMUS_HIPBLASLT_TUNING_STAGE`:

```bash
# default 0 means no tuning
export PRIMUS_HIPBLASLT_TUNING_STAGE=${PRIMUS_HIPBLASLT_TUNING_STAGE:-0}
```

---

### Stage 1: Dump GEMM Shape
In this stage, GEMM shapes used during training are collected.
It is recommended to reduce `train_iters` for faster shape generation.

```bash
# Output will be stored to:
# ./output/tune_hipblaslt/${PRIMUS_MODEL}/gemm_shape

export PRIMUS_HIPBLASLT_TUNING_STAGE=1
export EXP=examples/megatron/configs/llama2_7B-pretrain.yaml
NNODES=1 bash ./examples/run_slurm_pretrain.sh
```

---

### Stage 2: Tune GEMM Kernel

This stage performs kernel tuning based on the dumped GEMM shapes using the [offline_tune tool](https://github.com/AMD-AIG-AIMA/Primus/tree/main/examples/offline_tune).
It typically takes 10‚Äì30 minutes depending on model size and shape complexity.


```bash
# Output will be stored to:
# ./output/tune_hipblaslt/${PRIMUS_MODEL}/gemm_tune/tune_hipblas_gemm_results.txt

export PRIMUS_HIPBLASLT_TUNING_STAGE=2
export EXP=examples/megatron/configs/llama2_7B-pretrain.yaml
NNODES=1 bash ./examples/run_slurm_pretrain.sh
```

---

### Stage 3: Train with Tuned Kernel

In this final stage, the tuned kernel is loaded for efficient training:

```bash
export PRIMUS_HIPBLASLT_TUNING_STAGE=3
export EXP=examples/megatron/configs/llama2_7B-pretrain.yaml
NNODES=1 bash ./examples/run_slurm_pretrain.sh
```

## ‚úÖ Supported Models

The following models are supported out of the box via provided configuration files:

| Model            | Huggingface Config | Megatron Config | TorchTatin Config |
| ---------------- | ------------------ | --------------- | ----------------- |
| llama2_7B        | [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)         | [llama2_7B-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/config/llama2_7B-pretrain.yaml)               | |
| llama2_70B       | [meta-llama/Llama-2-70b-hf](https://huggingface.co/meta-llama/Llama-2-70b-hf)       | [llama2_70B-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/llama2_70B-pretrain.yaml)             | |
| llama3_8B        | [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)     | [llama3_8B-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/llama3_8B-pretrain.yaml)               | [llama3_8b.toml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/torchtatin/configs/llama3_8b.toml)
| llama3_70B       | [meta-llama/Meta-Llama-3-70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B)   | [llama3_70B-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/llama3_70B-pretrain.yaml)             | |
| llama3.1_8B      | [meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)           | [llama3.1_8B-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/llama3.1_8B-pretrain.yaml)           | |
| llama3.1_70B     | [meta-llama/Llama-3.1-70B](https://huggingface.co/meta-llama/Llama-3.1-70B)         | [llama3.1_70B-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/llama3.1_70B-pretrain.yaml)         | |
| deepseek_v2_lite | [deepseek-ai/DeepSeek-V2-Lite](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite) | [deepseek_v2_lite-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/deepseek_v2_lite-pretrain.yaml) | |
| deepseek_v2      | [deepseek-ai/DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)           | [deepseek_v2-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/deepseek_v2-pretrain.yaml)           | |
| deepseek_v3      | [deepseek-ai/DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)           | [deepseek_v3-pretrain.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/examples/megatron/configs/deepseek_v3-pretrain.yaml)           | |


---

### üèÉ‚Äç‚ôÇÔ∏è How to Run a Supported Model

Use the following command pattern to start training with a selected model configuration:

```bash
EXP=examples/megatron/configs/<model_config> bash ./examples/run_local_pretrain.sh
```

For example, to run the llama3_8B model quickly:

```bash
BACKEND=megatron EXP=examples/megatron/configs/llama3_8B-pretrain.yaml bash ./examples/run_local_pretrain.sh

BACKEND=torchtatin EXP=examples/torchtatin/configs/llama3_8b.toml bash ./examples/run_local_pretrain.sh
```


For multi-node training via SLURM, use:

```bash
export NNODES=8

#run megatron
BACKEND=megatron EXP=examples/megatron/configs/llama2_7B-pretrain.yaml bash ./examples/run_slurm_pretrain.sh

# run torchtatin
BACKEND=torchtatin EXP=examples/torchtatin/configs/llama3_8b.toml bash ./examples/run_slurm_pretrain.sh
```

## ‚ò∏Ô∏è Kubernetes Training Management (`run_k8s_pretrain.sh`)

he `run_k8s_pretrain.sh` script provides convenient CLI commands to manage training workloads on a Kubernetes cluster via a REST API. It supports creating, querying, deleting training jobs, and listing cluster nodes, facilitating flexible workload control for distributed training with Primus or similar frameworks.

### Requirements

- `jq` installed (for JSON processing)
- Access to Kubernetes API endpoint URL

### Usage

```bash
./run_k8s_pretrain.sh --url <api_base_url> <command> [options]

```



### ‚öôÔ∏è Commands

Primus provides several command-line interfaces to manage training workloads and cluster resources. Below are the commonly used commands:

| Command | Description                    |
| ------- | ------------------------------|
| create  | Create a new training workload |
| get     | Retrieve workload details      |
| delete  | Delete an existing workload    |
| list    | List all current workloads     |
| nodes   | List all nodes in the cluster  |

Use these commands to interact with Primus for workload scheduling and resource management.


---

### ‚öôÔ∏è Create Command Options

When using the `create` command to start a new training workload, the following options are supported:

| Option       | Description                                          | Default                                  |
| ------------ | ---------------------------------------------------- | ---------------------------------------- |
| `--replica`    | Number of replicas (instances)                       | 1                                        |
| `--cpu`        | Number of CPUs                                       | 96                                       |
| `--gpu`        | Number of GPUs                                       | 8                                        |
| `--backend`    | Training backend, e.g., `megatron` or `torchtitan`   | `megatron`                               |
| `--exp`        | Path to experiment (training config) file (required) | ‚Äî                                        |
| `--data_path`  | Path to training data                                | ‚Äî                                        |
| `--image`      | Docker image to use                                  | `docker.io/rocm/megatron-lm:v25.5_py310` |
| `--hf_token`   | HuggingFace token                                    | Read from env var `HF_TOKEN`             |
| `--workspace`  | Workspace name                                       | `primus-safe-pretrain`                   |
| `--nodelist`   | Comma-separated list of node hostnames to run on     | ‚Äî                                        |

### Example

Create a training workload with 2 replicas and custom config:


```bash
bash examples/run_k8s_pretrain.sh --url http://api.example.com create --replica 2 --cpu 96 --gpu 4 \
  --exp examples/megatron/configs/llama2_7B-pretrain.yaml --data_path /mnt/data/train \
  --image docker.io/custom/image:latest --hf_token myhf_token --workspace team-dev

#result:
{
  "workloadId": "abc123"
}

```

Get workload details:

```bash
bash examples/run_k8s_pretrain.sh --url http://api.example.com get --workload-id abc123

```

Delete a workload:

```bash
bash examples/run_k8s_pretrain.sh --url http://api.example.com delete --workload-id abc123

```

List all workloads:

```bash
bash examples/run_k8s_pretrain.sh --url http://api.example.com list

```

List all cluster nodes:

```bash
bash examples/run_k8s_pretrain.sh --url http://api.example.com nodes

```
